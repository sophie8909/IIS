{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affect Reconition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "from feat import Detector\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "from feat.utils import FEAT_EMOTION_COLUMNS\n",
    "from feat.pretrained import AU_LANDMARK_MAP\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Face model: retinaface\n",
      "c:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\face_detectors\\Retinaface\\Retinaface_test.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\n",
      "INFO:root:Loading Facial Landmark model: mobilefacenet\n",
      "c:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\detector.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "INFO:root:Loading facepose model: img2pose\n",
      "c:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_test.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n",
      "INFO:root:Loading AU model: xgb\n",
      "INFO:root:Loading emotion model: resmasknet\n",
      "c:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\emo_detectors\\ResMaskNet\\resmasknet_test.py:718: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unexpected EOF, expected 26690469 more bytes. The file might be corrupted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\detector.py:121\u001b[0m, in \u001b[0;36mDetector.__init__\u001b[1;34m(self, face_model, landmark_model, au_model, emotion_model, facepose_model, device, n_jobs, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Verify model names and download if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m face, landmark, au, emotion, facepose \u001b[38;5;241m=\u001b[39m get_pretrained_models(\n\u001b[0;32m    118\u001b[0m     face_model, landmark_model, au_model, emotion_model, facepose_model, verbose\n\u001b[0;32m    119\u001b[0m )\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_detectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlandmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43memotion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacepose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenface_2d_landmark_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\detector.py:297\u001b[0m, in \u001b[0;36mDetector._init_detectors\u001b[1;34m(self, face, landmark, au, emotion, facepose, openface_2d_landmark_columns, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_model\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m emotion\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memotion_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memotion_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43memotion_model_kwargs\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_model_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m FEAT_EMOTION_COLUMNS\n\u001b[0;32m    301\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(np\u001b[38;5;241m.\u001b[39matleast_2d(FEAT_EMOTION_COLUMNS), np\u001b[38;5;241m.\u001b[39mnan)\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\feat\\emo_detectors\\ResMaskNet\\resmasknet_test.py:718\u001b[0m, in \u001b[0;36mResMaskNet.__init__\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m=\u001b[39m (configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m resmasking_dropout1(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m--> 718\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_resource_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResMaskNet_Z_resmasking_dropout1_rot30.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sophie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1647\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[0;32m   1646\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1647\u001b[0m \u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1654\u001b[0m     offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unexpected EOF, expected 26690469 more bytes. The file might be corrupted."
     ]
    }
   ],
   "source": [
    "detector = Detector(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# emotion read as string\n",
    "df = pd.read_csv(\"dataset.csv\", dtype={\"emotion\": str})\n",
    "\n",
    "\n",
    "\n",
    "# get the list of emotions\n",
    "emotions = df['emotion'].unique()\n",
    "\n",
    "# encode the emotions as integers\n",
    "emotion_to_int = {emotion: i for i, emotion in enumerate(emotions)}\n",
    "\n",
    "# features: AU01,AU02,AU04,AU05,AU06,AU07,AU09,AU10,AU11,AU12,AU14,AU15,AU17,AU20,AU23,AU24,AU25,AU26,AU28,AU43\n",
    "# labels: emotion\n",
    "features = df[AU_LANDMARK_MAP[\"Feat\"]].values\n",
    "labels = df['emotion'].map(emotion_to_int).values\n",
    "\n",
    "# convert to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Train/val/test split percentage: 70/20/10\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.2 * len(df))\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "# split the data\n",
    "train_features, val_features, test_features = features[:train_size], features[train_size:train_size+val_size], features[train_size+val_size:]\n",
    "train_labels, val_labels, test_labels = labels[:train_size], labels[train_size:train_size+val_size], labels[train_size+val_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (812, 20)\n",
      "Train labels shape: (812,)\n",
      "Val features shape: (232, 20)\n",
      "Val labels shape: (232,)\n",
      "Test features shape: (117, 20)\n",
      "Test labels shape: (117,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train features shape:\", train_features.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Val features shape:\", val_features.shape)\n",
    "print(\"Val labels shape:\", val_labels.shape)\n",
    "print(\"Test features shape:\", test_features.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn model: Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for SVC (kernel='linear'): 0.5862068965517241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVC with linear kernel\n",
    "# train the model\n",
    "model_SVC_linear = SVC(kernel='linear')\n",
    "model_SVC_linear.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_SVC_linear.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for SVC (kernel='linear'): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for SVC (kernel='rbf'): 0.5905172413793104\n"
     ]
    }
   ],
   "source": [
    "# SVC with RBF kernel\n",
    "# train the model\n",
    "model_SVC_rbf = SVC(kernel='rbf')\n",
    "model_SVC_rbf.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_SVC_rbf.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for SVC (kernel='rbf'): {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for SVC (kernel='poly'): 0.5862068965517241\n"
     ]
    }
   ],
   "source": [
    "# SVC with poly kernel\n",
    "# train the model\n",
    "model_SVC_poly = SVC(kernel='poly')\n",
    "model_SVC_poly.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_SVC_poly.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for SVC (kernel='poly'): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for SVC (kernel='sigmoid'): 0.2025862068965517\n"
     ]
    }
   ],
   "source": [
    "# SVC with sigmoid kernel\n",
    "# train the model\n",
    "model_SVC_sigmoid = SVC(kernel='sigmoid')\n",
    "model_SVC_sigmoid.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_SVC_sigmoid.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for SVC (kernel='sigmoid'): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for SVC (kernel='precomputed'): 0.5862068965517241\n"
     ]
    }
   ],
   "source": [
    "# SVC with precomputed kernel\n",
    "# train the model\n",
    "model_SVC_precomputed = SVC(kernel='precomputed')\n",
    "model_SVC_precomputed.fit(train_features @ train_features.T, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_SVC_precomputed.predict(val_features @ train_features.T)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for SVC (kernel='precomputed'): {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for SVC (kernel='rbf'): 0.5982905982905983\n"
     ]
    }
   ],
   "source": [
    "# SVC test accuracy with rbf kernel\n",
    "test_predictions = model_SVC_rbf.predict(test_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy for SVC (kernel='rbf'): {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn model: K-nearest neighbor(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for KNN (k=1): 0.5387931034482759\n",
      "Validation Accuracy for KNN (k=3): 0.5517241379310345\n",
      "Validation Accuracy for KNN (k=5): 0.5301724137931034\n",
      "Validation Accuracy for KNN (k=7): 0.5646551724137931\n",
      "Validation Accuracy for KNN (k=9): 0.5301724137931034\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN with k=1\n",
    "# train the model\n",
    "\n",
    "model_KNN_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "model_KNN_1.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_KNN_1.predict(val_features)\n",
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for KNN (k=1): {accuracy}\")\n",
    "\n",
    "# KNN with k=3\n",
    "# train the model\n",
    "model_KNN_3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model_KNN_3.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_KNN_3.predict(val_features)\n",
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for KNN (k=3): {accuracy}\")\n",
    "\n",
    "\n",
    "# KNN with k=5\n",
    "# train the model\n",
    "model_KNN_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_KNN_5.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_KNN_5.predict(val_features)\n",
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for KNN (k=5): {accuracy}\")\n",
    "\n",
    "# KNN with k=7\n",
    "# train the model\n",
    "model_KNN_7 = KNeighborsClassifier(n_neighbors=7)\n",
    "model_KNN_7.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_KNN_7.predict(val_features)\n",
    "\n",
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for KNN (k=7): {accuracy}\")\n",
    "# KNN with k=9\n",
    "# train the model\n",
    "model_KNN_9 = KNeighborsClassifier(n_neighbors=9)\n",
    "model_KNN_9.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_KNN_9.predict(val_features)\n",
    "\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for KNN (k=9): {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for KNN (k=7): 0.5897435897435898\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "# KNN test accuracy with k=7\n",
    "\n",
    "test_predictions = model_KNN_7.predict(test_features)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy for KNN (k=7): {test_accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn model: (MPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPL\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# MLP with 1 hidden layer\n",
    "# train the model\n",
    "model_MLP_1 = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "model_MLP_1.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_MLP_1.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for MLP (1 hidden layer): {accuracy}\")\n",
    "\n",
    "# MLP with 2 hidden layers\n",
    "# train the model\n",
    "model_MLP_2 = MLPClassifier(hidden_layer_sizes=(100, 100))\n",
    "model_MLP_2.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_MLP_2.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for MLP (2 hidden layers): {accuracy}\")\n",
    "\n",
    "# MLP with 3 hidden layers\n",
    "# train the model\n",
    "model_MLP_3 = MLPClassifier(hidden_layer_sizes=(100, 100, 100))\n",
    "model_MLP_3.fit(train_features, train_labels)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = model_MLP_3.predict(val_features)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f\"Validation Accuracy for MLP (3 hidden layers): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
